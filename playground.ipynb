{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# old but gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Function to draw bounding boxes on an image\n",
    "def draw_boxes(image, annotation_data):\n",
    "    for person_data in annotation_data:\n",
    "        views = person_data[\"views\"]\n",
    "        for view in views:\n",
    "            if view['viewNum'] != 1:\n",
    "                continue\n",
    "            assert False\n",
    "            xmin, ymin, xmax, ymax = view[\"xmin\"], view[\"ymin\"], view[\"xmax\"], view[\"ymax\"]\n",
    "            cv2.rectangle(image, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n",
    "\n",
    "# Path to the folder containing frame images\n",
    "image_folder = \"/ssd/r.musaev/deepsort/data/Wildtrack_dataset/Image_subsets/C7/\"\n",
    "\n",
    "# Join the annotated images into a video clip\n",
    "video_writer = cv2.VideoWriter(\"/ssd/r.musaev/deepsort/input/Wildtrack/full/camera_6.mp4\", cv2.VideoWriter_fourcc(*'mp4v'), 10, (1920, 1080))\n",
    "\n",
    "# Iterate through all frames\n",
    "for filename in sorted(os.listdir(image_folder)):\n",
    "    if filename.endswith(\".png\"):\n",
    "        frame_number = filename.split(\".\")[0]\n",
    "        image_path = os.path.join(image_folder, filename)\n",
    "        annotation_path = os.path.join(\"/ssd/r.musaev/deepsort/data/Wildtrack_dataset/annotations_positions/\", f\"{frame_number}.json\")\n",
    "\n",
    "        if not os.path.isfile(annotation_path):\n",
    "            break\n",
    "        # Load annotation data from JSON file\n",
    "        with open(annotation_path, \"r\") as f:\n",
    "            annotation_data = json.load(f)\n",
    "\n",
    "        # Load the frame image\n",
    "        image = cv2.imread(image_path)\n",
    "\n",
    "        # # Draw bounding boxes on the image\n",
    "        # draw_boxes(image, annotation_data)\n",
    "\n",
    "        # Write the annotated frame to the video clip\n",
    "        video_writer.write(image)\n",
    "\n",
    "video_writer.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def read_video_clip(video_path, max_frame):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = np.ceil(cap.get(cv2.CAP_PROP_FPS))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    \n",
    "    # Determine the maximum number of frames to read\n",
    "    max_frame = min(max_frame, frame_count)\n",
    "\n",
    "    frames = []\n",
    "    current_frame = 0\n",
    "\n",
    "    while current_frame < max_frame:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frames.append(frame)\n",
    "        current_frame += 1\n",
    "\n",
    "    cap.release()\n",
    "    return frames, fps, width, height\n",
    "\n",
    "def draw_bbox(frame, bbox):\n",
    "    # Extract bbox coordinates\n",
    "    x, y, w, h = bbox\n",
    "    # Draw bbox on the frame\n",
    "    cv2.rectangle(frame, (int(x), int(y)), (int(x + w), int(y + h)), (0, 255, 0), 2)\n",
    "\n",
    "# Read the video clip up to the specified maximum frame\n",
    "max_frame = 500\n",
    "video_path = '/ssd/r.musaev/deepsort/data/Wildtrack_dataset/cam2.mp4'\n",
    "frames, fps, width, height = read_video_clip(video_path, max_frame)\n",
    "\n",
    "# Path to the annotation file\n",
    "annotation_file = '/ssd/r.musaev/deepsort/input/Wildtrack/subset/camera_6.txt /ssd/r.musaev/deepsort/input/Wildtrack/subset/camera_5.txt /ssd/r.musaev/deepsort/input/Wildtrack/subset/camera_2.txt /ssd/r.musaev/deepsort/input/Wildtrack/subset/camera_4.txt /ssd/r.musaev/deepsort/input/Wildtrack/subset/camera_3.txt /ssd/r.musaev/deepsort/input/Wildtrack/subset/camera_1.txt /ssd/r.musaev/deepsort/input/Wildtrack/subset/camera_0.txt/camera_1.txt'\n",
    "\n",
    "# Read annotations\n",
    "annotations = []\n",
    "with open(annotation_file, 'r') as f:\n",
    "    for line in f:\n",
    "        data = line.strip().split(',')\n",
    "        frame_num = int(data[0])\n",
    "        bbox = list(map(int, data[2:6]))\n",
    "        annotations.append((frame_num, bbox))\n",
    "\n",
    "# Draw bounding boxes on frames\n",
    "for frame_num, bbox in annotations:\n",
    "    if frame_num >= max_frame:\n",
    "            break\n",
    "    draw_bbox(frames[frame_num], bbox)\n",
    "\n",
    "# Join frames into video\n",
    "output_video_path = '/ssd/r.musaev/deepsort/data/Wildtrack_dataset/cam2_annotated.mp4'\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "\n",
    "for frame in frames:\n",
    "    out.write(frame)\n",
    "\n",
    "out.release()\n",
    "print(\"Video saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "            \n",
    "fps_video = 60\n",
    "fps_anno = 2\n",
    "FPS_RATIO = 1 #6 #fps_video / fps_anno\n",
    "\n",
    "\n",
    "def convert_annotations(json_files):\n",
    "    cameras = {}\n",
    "    for frame_num, json_file in enumerate(json_files):\n",
    "        with open(json_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        #frame_num = int(int(json_file.split('/')[-1].split('.')[0]) * FPS_RATIO)\n",
    "        for person_data in data:\n",
    "            for view_data in person_data['views']:\n",
    "                if view_data['xmin'] == -1:\n",
    "                    continue\n",
    "                view_num = view_data['viewNum']\n",
    "                if view_num not in cameras:\n",
    "                    cameras[view_num] = []\n",
    "                view_data['xmax'] -= view_data['xmin']\n",
    "                view_data['ymax'] -= view_data['ymin']\n",
    "                cameras[view_num].append({\n",
    "                    'fr_num': frame_num,\n",
    "                    'personID': person_data['personID'],\n",
    "                    'boxes_coord': (view_data['xmin'], view_data['ymin'], view_data['xmax'], view_data['ymax'])\n",
    "                })\n",
    "    \n",
    "    return cameras\n",
    "\n",
    "def write_mot_files(annotations_dir, output_dir):\n",
    "    json_files = [os.path.join(annotations_dir, f) for f in os.listdir(annotations_dir) if f.endswith('.json')]\n",
    "    cameras = convert_annotations(json_files)\n",
    "    \n",
    "    for view_num, view_data in cameras.items():\n",
    "        view_data.sort(key=lambda x: x['personID'])  # Sort detections by person ID\n",
    "        output_file = os.path.join(output_dir, f'camera_{view_num}.txt')\n",
    "        with open(output_file, 'w') as f:\n",
    "            for frame_num, person_data in enumerate(view_data, start=1):\n",
    "                f.write(f\"{person_data['fr_num']} {person_data['personID']} {' '.join(map(str, person_data['boxes_coord']))} -1 -1\\n\")\n",
    "\n",
    "# Provide the input and output directories\n",
    "annotations_dir = '/ssd/r.musaev/deepsort/data/Wildtrack_dataset/annotations_positions'\n",
    "output_dir = '/ssd/r.musaev/deepsort/input/Wildtrack/full'\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Convert annotations and write MOT files\n",
    "write_mot_files(annotations_dir, output_dir)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "path = '/ssd/r.musaev/deepsort/input/Wildtrack/full/camera_*.txt'\n",
    "for path in glob.glob(path):\n",
    "    df = pd.read_csv(path, names=[\n",
    "                                'fr_num', 'id', 'bbleft', 'bbtop',\n",
    "                                'bbox_width', 'bbox_height', 'det_score',\n",
    "                                'class'], index_col=False, sep=' ')\n",
    "\n",
    "    df = df.sort_values(by=['fr_num', 'id'])\n",
    "    df.to_csv(path, index=False, header=None, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ssd/r.musaev/python/miniconda3/envs/mcmt/lib/python3.9/site-packages/scipy/interpolate/_interpolate.py:710: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# Read annotations from the text file\n",
    "annotations_file = '/ssd/r.musaev/deepsort/data/Wildtrack_dataset/annotations_positions/camera_0.txt'\n",
    "annotations_data = np.genfromtxt(annotations_file, delimiter=',', skip_header=1)\n",
    "\n",
    "# Extract relevant information\n",
    "frame_nums = annotations_data[:, 0]\n",
    "ids = annotations_data[:, 1]\n",
    "bboxes = annotations_data[:, 2:6]  # Extracting bbox_x, bbox_y, bbox_w, bbox_h\n",
    "\n",
    "# Determine fps and number of frames\n",
    "fps = 2\n",
    "num_frames = int(np.max(frame_nums)) + 1\n",
    "\n",
    "# Perform interpolation for each coordinate separately\n",
    "interpolated_annotations = np.zeros((num_frames * fps, int(np.max(ids)) + 1, 4))  # Assuming ids are consecutive and start from 0\n",
    "for i in range(4):  # Iterate over (x, y, width, height)\n",
    "    interpolated_coords = np.zeros((num_frames * fps, int(np.max(ids)) + 1))\n",
    "    for idx in np.unique(ids):\n",
    "        # Extract coordinates for a specific id\n",
    "        id_mask = ids == idx\n",
    "        id_frame_nums = frame_nums[id_mask]\n",
    "        id_bboxes = bboxes[id_mask, i]\n",
    "\n",
    "        # Create interpolation function for each id\n",
    "        interp_func = interp1d(id_frame_nums, id_bboxes, kind='linear', axis=0, fill_value=\"extrapolate\")\n",
    "        # Interpolate coordinates for each id\n",
    "        interpolated_coords[:, int(idx)] = interp_func(np.arange(num_frames * fps) / fps)\n",
    "\n",
    "    interpolated_annotations[:, :, i] = interpolated_coords\n",
    "\n",
    "# Result will be in interpolated_annotations array\n",
    "# Define the output MOT file name\n",
    "output_file = annotations_file.replace('.txt', '_interpolated.txt')\n",
    "\n",
    "# Write interpolated annotations to the MOT file\n",
    "with open(output_file, 'w') as f:\n",
    "    # Write header\n",
    "    f.write(\"frame_num,id,bbox_x,bbox_y,bbox_w,bbox_h,-1,-1\\n\")\n",
    "    \n",
    "    # Write interpolated annotations\n",
    "    for frame_num in range(num_frames * fps):\n",
    "        for idx in range(interpolated_annotations.shape[1]):\n",
    "            bbox_x, bbox_y, bbox_w, bbox_h = interpolated_annotations[frame_num, idx]\n",
    "            f.write(f\"{frame_num},{idx},{bbox_x},{bbox_y},{bbox_w},{bbox_h},-1,-1\\n\")\n",
    "\n",
    "print(f\"Interpolated annotations saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2924630/3398986690.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "/tmp/ipykernel_2924630/3398986690.py:4: DtypeWarning: Columns (0,1,2,3,4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  pred = pd.read_csv(path, names=[\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "path = '/ssd/r.musaev/deepsort/data/Wildtrack_dataset/annotations_positions/camera_0_interpolated.txt'\n",
    "pred = pd.read_csv(path, names=[\n",
    "    'fr_num', 'id', 'bbleft', 'bbtop',\n",
    "    'bbox_width', 'bbox_height', 'det_score',\n",
    "    'class'],\n",
    "                    index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA_VISIBLE_DEVICES=0 python3 /ssd/r.musaev/mmdetection/demo/mot_demo.py /ssd/r.musaev/deepsort/data/mvmhat_1/1/images bytetrack_yolox_x_8xb4-amp-80e_crowdhuman-mot17halftrain_test-mot17halfval.py --checkpoint rtmdet_tiny_8xb32-300e_coco_20220902_112414-78e30dcc.pth --out /ssd/r.musaev/deepsort/output_mmtracking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python demo/demo_mot_vis.py \\\n",
    "#     configs/vis/masktrack_rcnn/masktrack_rcnn_r50_fpn_12e_youtubevis2019.py \\\n",
    "#     --input /ssd/r.musaev/deepsort/data/Wildtrack_dataset/cam1_clip01.mp4 \\\n",
    "#     --checkpoint checkpoints/masktrack_rcnn_r50_fpn_12e_youtubevis2019_20211022_194830-6ca6b91e.pth \\\n",
    "#     --output /ssd/r.musaev/deepsort/output_mmtracking.mp4 \\\n",
    "#     --show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision.transforms import ToTensor\n",
    "# import cv2\n",
    "# import torch\n",
    "# from PIL import Image\n",
    "# import numpy as np\n",
    "\n",
    "# path = '/ssd/r.musaev/deepsort/data/Wildtrack_dataset/Image_subsets/C2/00000000.png'\n",
    "# frame = np.asarray(Image.open(path))\n",
    "\n",
    "# resized_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "# # Convert frame to tensor and send it to device (cpu or cuda)\n",
    "# frame_tensor = ToTensor()(resized_frame)\n",
    "\n",
    "# from mmdet.apis import init_detector, inference_detector\n",
    "\n",
    "# config_file = 'rtmdet_tiny_8xb32-300e_coco.py'\n",
    "# checkpoint_file = 'rtmdet_tiny_8xb32-300e_coco_20220902_112414-78e30dcc.pth'\n",
    "# model = init_detector(config_file, checkpoint_file, device='cpu')  # or device='cuda:0'\n",
    "# detecions = inference_detector(model, frame_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # # build the model from a config file and a checkpoint file\n",
    "    # model = init_track_model(\n",
    "    #     args.config,\n",
    "    #     args.checkpoint,\n",
    "    #     args.detector,\n",
    "    #     args.reid,\n",
    "    #     device=args.device)\n",
    "\n",
    "    # # build the visualizer\n",
    "    # visualizer = VISUALIZERS.build(model.cfg.visualizer)\n",
    "    # visualizer.dataset_meta = model.dataset_meta\n",
    "\n",
    "    # prog_bar = mmengine.ProgressBar(len(imgs))\n",
    "    # # test and show/save the images\n",
    "    # for i, img in enumerate(imgs):\n",
    "    #     if isinstance(img, str):\n",
    "    #         img_path = osp.join(args.inputs, img)\n",
    "    #         img = mmcv.imread(img_path)\n",
    "    #     # result [TrackDataSample]\n",
    "    #     result = inference_mot(model, img, frame_id=i, video_len=len(imgs))\n",
    "    #     if args.out is not None:\n",
    "    #         if in_video or out_video:\n",
    "    #             out_file = osp.join(out_path, f'{i:06d}.jpg')\n",
    "    #         else:\n",
    "    #             out_file = osp.join(out_path, img.rsplit(os.sep, 1)[-1])\n",
    "    #     else:\n",
    "    #         out_file = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# def get_gt_bboxes(gt_camera, frame_count):\n",
    "#     \"\"\"look up for gt_bboxes on frame_count frame\"\"\"\n",
    "\n",
    "#     gt_bboxes = gt_camera[gt_camera.fr_num == frame_count].iloc[:, 2:6].values\n",
    "#     gt_ids = gt_camera[gt_camera.fr_num == frame_count].iloc[:, 1].values\n",
    "\n",
    "#     return gt_bboxes, gt_ids\n",
    "\n",
    "# path = '/ssd/r.musaev/deepsort/input/AIC22_Track1_MTMC_Tracking/S02c006.txt'\n",
    "# pred = pd.read_csv(path, names=[\n",
    "#     'fr_num', 'id', 'bbleft', 'bbtop',\n",
    "#     'bbox_width', 'bbox_height', 'det_score',\n",
    "#     'class', 'visibility', 'stuff'],\n",
    "#                     index_col=False)\n",
    "\n",
    "# frame_count = 0\n",
    "# gt_bboxes, gt_ids = get_gt_bboxes(pred, frame_count)\n",
    "# len(gt_bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def loadroi(imf):\n",
    "    \"\"\"Read the ROI image for a given camera.\n",
    "\n",
    "    Params\n",
    "    ------\n",
    "    cid : int\n",
    "        Camera ID whose ROI image should be retrieved.\n",
    "    Returns\n",
    "    -------\n",
    "    im : numpy.ndarray\n",
    "        Image stored as a 2-d ndarray.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # img = Image.open(imf, mode='r')\n",
    "    img = np.asarray(Image.open(imf))\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "imf = '/ssd/r.musaev/deepsort/data/AIC22_Track1_MTMC_Tracking/validation/S02/c006/roi.jpg'\n",
    "img = loadroi(imf)\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isROIOutlier(row, roi, height, width):\n",
    "    \"\"\"Check whether item stored in row is outside the region of interest.\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    row : pandas.Series\n",
    "        Row of data including, at minimum, the 'X', 'Y', 'Width', and 'Height' columns.\n",
    "    roi : numpy.ndarray\n",
    "        ROI image for the camera with the same id as in row['CameraId'].\n",
    "    height : int\n",
    "        ROI image height.\n",
    "    width : int\n",
    "        ROI image width.\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        Return True if image is an outlier.\n",
    "    \"\"\"\n",
    "    xmin = row['X']\n",
    "    ymin = row['Y']\n",
    "    xmax = row['X'] + row['Width']\n",
    "    ymax = row['Y'] + row['Height']\n",
    "\n",
    "    if xmin >= 0 and xmin < width:\n",
    "        if ymin >= 0 and ymin < height and roi[ymin, xmin] < 255:\n",
    "            return True\n",
    "        if ymax >= 0 and ymax < height and roi[ymax, xmin] < 255:\n",
    "            return True\n",
    "    if xmax >= 0 and xmax < width:\n",
    "        if ymin >= 0 and ymin < height and roi[ymin, xmax] < 255:\n",
    "            return True\n",
    "        if ymax >= 0 and ymax < height and roi[ymax, xmax] < 255:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES= python3 eval.py --input_folder /ssd/r.musaev/deepsort/input/Wildtrack/full --mode fasterrcnn_resnet50_fpn_v2 --embedder torchreid  --global_reid_model_wts osnet_ibn_x1_0_msmt17_combineall_256x128_amsgrad_ep150_stp60_lr0.0015_b64_fb10_softmax_labelsmooth_flip_jitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=2 python3 eval.py --input_folder /ssd/r.musaev/deepsort/data/mvmhat_1 --model fasterrcnn_resnet50_fpn_v2 --embedder torchreid --global_reid_model_wts osnet_ibn_x1_0_msmt17_combineall_256x128_amsgrad_ep150_stp60_lr0.0015_b64_fb10_softmax_labelsmooth_flip_jitter --nms_max_overlap 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "; CUDA_VISIBLE_DEVICES=1 python3 eval.py --input_folder /ssd/r.musaev/deepsort/data/mvmhat_1 --model fasterrcnn_resnet50_fpn_v2 --embedder clip_RN50x4; CUDA_VISIBLE_DEVICES=1 python3 eval.py --input_folder /ssd/r.musaev/deepsort/data/mvmhat_1 --model fasterrcnn_resnet50_fpn_v2 --embedder clip_RN50x16; CUDA_VISIBLE_DEVICES=1 python3 eval.py --input_folder /ssd/r.musaev/deepsort/data/mvmhat_1 --model fasterrcnn_resnet50_fpn_v2 --embedder clip_ViT-B/32; CUDA_VISIBLE_DEVICES=1 python3 eval.py --input_folder /ssd/r.musaev/deepsort/data/mvmhat_1 --model fasterrcnn_resnet50_fpn_v2 --embedder clip_ViT-B/16;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# detectors grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=1 python3 eval.py --input_folder /ssd/r.musaev/deepsort/data/mvmhat_1 --model fasterrcnn_resnet50_fpn --embedder mobilenet --global_reid_model_wts osnet_ibn_x1_0_msmt17_combineall_256x128_amsgrad_ep150_stp60_lr0.0015_b64_fb10_softmax_labelsmooth_flip_jitter; CUDA_VISIBLE_DEVICES=1 python3 eval.py --input_folder /ssd/r.musaev/deepsort/data/mvmhat_1 --model fasterrcnn_mobilenet_v3_large_fpn --embedder mobilenet --global_reid_model_wts osnet_ibn_x1_0_msmt17_combineall_256x128_amsgrad_ep150_stp60_lr0.0015_b64_fb10_softmax_labelsmooth_flip_jitter; CUDA_VISIBLE_DEVICES=1 python3 eval.py --input_folder /ssd/r.musaev/deepsort/data/mvmhat_1 --model fasterrcnn_mobilenet_v3_large_320_fpn --embedder mobilenet --global_reid_model_wts osnet_ibn_x1_0_msmt17_combineall_256x128_amsgrad_ep150_stp60_lr0.0015_b64_fb10_softmax_labelsmooth_flip_jitter; CUDA_VISIBLE_DEVICES=1 python3 eval.py --input_folder /ssd/r.musaev/deepsort/data/mvmhat_1 --model fcos_resnet50_fpn --embedder mobilenet --global_reid_model_wts osnet_ibn_x1_0_msmt17_combineall_256x128_amsgrad_ep150_stp60_lr0.0015_b64_fb10_softmax_labelsmooth_flip_jitter; CUDA_VISIBLE_DEVICES=1 python3 eval.py --input_folder /ssd/r.musaev/deepsort/data/mvmhat_1 --model ssd300_vgg16 --embedder mobilenet --global_reid_model_wts osnet_ibn_x1_0_msmt17_combineall_256x128_amsgrad_ep150_stp60_lr0.0015_b64_fb10_softmax_labelsmooth_flip_jitter; CUDA_VISIBLE_DEVICES=1 python3 eval.py --input_folder /ssd/r.musaev/deepsort/data/mvmhat_1 --model ssdlite320_mobilenet_v3_large --embedder mobilenet --global_reid_model_wts osnet_ibn_x1_0_msmt17_combineall_256x128_amsgrad_ep150_stp60_lr0.0015_b64_fb10_softmax_labelsmooth_flip_jitter; CUDA_VISIBLE_DEVICES=1 python3 eval.py --input_folder /ssd/r.musaev/deepsort/data/mvmhat_1 --model retinanet_resnet50_fpn --embedder mobilenet --global_reid_model_wts osnet_ibn_x1_0_msmt17_combineall_256x128_amsgrad_ep150_stp60_lr0.0015_b64_fb10_softmax_labelsmooth_flip_jitter; CUDA_VISIBLE_DEVICES=1 python3 eval.py --input_folder /ssd/r.musaev/deepsort/data/mvmhat_1 --model retinanet_resnet50_fpn_v2 --embedder mobilenet --global_reid_model_wts osnet_ibn_x1_0_msmt17_combineall_256x128_amsgrad_ep150_stp60_lr0.0015_b64_fb10_softmax_labelsmooth_flip_jitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=1 python3 eval.py --input_folder /ssd/r.musaev/deepsort/data/mvmhat_1 --model fasterrcnn_resnet50_fpn --embedder mobilenet --global_reid_model_wts osnet_ibn_x1_0_msmt17_combineall_256x128_amsgrad_ep150_stp60_lr0.0015_b64_fb10_softmax_labelsmooth_flip_jitter ; CUDA_VISIBLE_DEVICES=1 python3 eval.py --input_folder /ssd/r.musaev/deepsort/data/mvmhat_1 --model fasterrcnn_resnet50_fpn --embedder torchreid --global_reid_model_wts osnet_ibn_x1_0_msmt17_combineall_256x128_amsgrad_ep150_stp60_lr0.0015_b64_fb10_softmax_labelsmooth_flip_jitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=1 python3 eval.py --input_folder /ssd/r.musaev/deepsort/data/mvmhat_1 --model fasterrcnn_mobilenet_v3_large_fpn --embedder mobilenet --global_reid_model_wts osnet_ibn_x1_0_imagenet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=1 python3 eval.py --input_folder /ssd/r.musaev/deepsort/input/AIC22_Track1_MTMC_Tracking  --mode fasterrcnn_resnet50_fpn_v2  --embedder torchreid  --global_reid_model_wts osnet_ibn_x1_0_msmt17_combineall_256x128_amsgrad_ep150_stp60_lr0.0015_b64_fb10_softmax_labelsmooth_flip_jitter --cls 2 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import shutil\n",
    "for src in sorted(glob.glob('/ssd/r.musaev/deepsort/data/AIC22_Track1_MTMC_Tracking/validation/S02/**/*.avi')):\n",
    "    dst = '/ssd/r.musaev/deepsort/input/AIC22_Track1_MTMC_Tracking/S02'\n",
    "    dst += src.split('S02/')[1].replace('/', '_').replace('avi','mp4')\n",
    "    dst = dst.replace('_vdo', '')\n",
    "\n",
    "    shutil.copyfile(src, dst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import shutil\n",
    "for src in sorted(glob.glob('/ssd/r.musaev/deepsort/data/AIC22_Track1_MTMC_Tracking/validation/S02/*/gt/gt.txt')):\n",
    "    print(src)\n",
    "    dst = '/ssd/r.musaev/deepsort/input/AIC22_Track1_MTMC_Tracking/S02'\n",
    "    dst += src.split('S02/')[1].replace('/', '_').replace('avi','mp4')\n",
    "    dst = dst.replace('mp4', 'txt')\n",
    "    dst = dst.replace('_gt_gt','')\n",
    "    shutil.copyfile(src, dst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(vector):\n",
    "    return vector / np.linalg.norm(vector)\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "gt_file = '/ssd/r.musaev/deepsort/outputs/52/S02c007_retinanet_resnet50_fpn_v2_torchreid_osnet_ibn_x1_0_imagenet.txt'\n",
    "df = pd.read_csv(gt_file, names=[\n",
    "                            'fr_num', 'id', 'bbleft', 'bbtop',\n",
    "                            'bbox_width', 'bbox_height', 'det_score',\n",
    "                            'class', 'visibility', 'stuff', 'desc_reid'], index_col=False)\n",
    "                        \n",
    "df.desc_reid = df.desc_reid.apply(lambda x: np.array(ast.literal_eval(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import motmetrics as mm\n",
    "\n",
    "def evaluate_tracker(preds, gt):\n",
    "\n",
    "    # Create MOTAccumulator object  \n",
    "    acc = mm.MOTAccumulator() \n",
    "\n",
    "    # Update with ground truth and predictions\n",
    "    for frame in range(1, len(gt) + 1):\n",
    "        gt_boxes = [[b[2], b[3], b[4], b[5]] for b in gt[gt.fr_num == frame].values]\n",
    "        hp_boxes = [[b[2], b[3], b[4], b[5]] for b in preds[preds.fr_num == frame].values]\n",
    "        print(gt_boxes)\n",
    "        print(hp_boxes)\n",
    "        \n",
    "        if gt_boxes and hp_boxes:\n",
    "            dists = mm.distances.iou_matrix(gt_boxes, hp_boxes, max_iou=0.5)\n",
    "        \n",
    "        else:\n",
    "            dists = np.zeros((0,0))\n",
    "\n",
    "        print(type(gt_boxes))\n",
    "\n",
    "        acc.update(\n",
    "            gt_boxes,  \n",
    "            hp_boxes, \n",
    "            dists,\n",
    "            frameid=frame)\n",
    "        \n",
    "        print(acc.events) \n",
    "            \n",
    "    # Get summary metrics  \n",
    "    mh = mm.metrics.create()\n",
    "    summary = mh.compute(acc, metrics=['num_frames', 'mota', 'motp'], name='acc')\n",
    "    \n",
    "    return summary  \n",
    "\n",
    "# Usage\n",
    "summary = evaluate_tracker(df, df)\n",
    "print(f\"MOTA: {summary['mota']}, MOTP: {summary['motp']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "VIDEO_PATH = '/ssd/r.musaev/deepsort/data/mvmhat_1/mvmhat_1_1.mp4'\n",
    "gt = pd.read_csv(VIDEO_PATH.replace('mp4', 'txt'), names=[\n",
    "    'fr_num', 'id', 'bbleft', 'bbtop',\n",
    "    'bbox_width', 'bbox_height', 'det_score',\n",
    "    'class', 'visibility', 'stuff',],\n",
    "                    index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scale_gt_bbox(gt_bboxes, original_hw, target_hw):\n",
    "    \"\"\"\"applies scaling of gt-bboxes\"\"\"\n",
    "    \n",
    "    gt_bboxes.bbleft = gt_bboxes.bbleft.apply(lambda x: int(x * target_hw[1]/original_hw[1]))\n",
    "    gt_bboxes.bbtop = gt_bboxes.bbtop.apply(lambda x: int(x * target_hw[0]/original_hw[0]))\n",
    "    gt_bboxes.bbox_width = gt_bboxes.bbox_width.apply(lambda x: int(x * target_hw[1]/original_hw[1]))\n",
    "    gt_bboxes.bbox_height = gt_bboxes.bbox_height.apply(lambda x: int(x * target_hw[0]/original_hw[0]))\n",
    "    \n",
    "    return gt_bboxes\n",
    "\n",
    "VIDEO_PATH = '/ssd/r.musaev/deepsort/data/mvmhat_1/mvmhat_1_1.mp4'\n",
    "gt = pd.read_csv(VIDEO_PATH.replace('mp4', 'txt'), names=[\n",
    "    'fr_num', 'id', 'bbleft', 'bbtop',\n",
    "    'bbox_width', 'bbox_height', 'det_score',\n",
    "    'class', 'visibility', 'stuff',],\n",
    "                    index_col=False)\n",
    "\n",
    "original_hw = (1520, 2704)\n",
    "target_hw = (574, 1024)\n",
    "gt = scale_gt_bbox(gt, original_hw, target_hw)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxes = gt[gt.fr_num == 234].iloc[:, 2:6].values\n",
    "ids = gt[gt.fr_num == 234].iloc[:, 1].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nuscenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bcubed_eval_bcu(hyp_obj_pairs):\n",
    "    \"\"\"\n",
    "    Evaluate clustering using B-cubed metrics: precision, recall, and F1-score.\n",
    "\n",
    "    Params:\n",
    "        hyp_obj_pairs (list): List of tuples where:\n",
    "            (i_pred_cluster, j_gt_label) - correct assignment\n",
    "            (i_pred_cluster, -1) - false positive\n",
    "            (-1, j_gt_label) - false negative\n",
    "\n",
    "    Returns:\n",
    "        tuple: (precision, recall, fscore)\n",
    "    \"\"\"\n",
    "    # Initialize dictionaries to store counts\n",
    "    cluster_counts = {}\n",
    "    label_counts = {}\n",
    "    intersection_counts = {}\n",
    "\n",
    "    # Count assignments and occurrences\n",
    "    for cluster, label in hyp_obj_pairs:\n",
    "        if cluster != -1:\n",
    "            cluster_counts[cluster] = cluster_counts.get(cluster, 0) + 1\n",
    "        if label != -1:\n",
    "            label_counts[label] = label_counts.get(label, 0) + 1\n",
    "        if cluster != -1 and label != -1:\n",
    "            intersection_counts[(cluster, label)] = intersection_counts.get((cluster, label), 0) + 1\n",
    "\n",
    "    print(cluster_counts)\n",
    "    print(label_counts)\n",
    "    # Calculate precision and recall for each cluster\n",
    "    precisions = {}\n",
    "    recalls = {}\n",
    "    for cluster, label in hyp_obj_pairs:\n",
    "        if cluster == -1 or label == -1:\n",
    "            continue\n",
    "        tp = intersection_counts.get((cluster, label), 0)\n",
    "        precision = tp / cluster_counts[cluster]\n",
    "        recall = tp / label_counts[label]\n",
    "        precisions[cluster] = precisions.get(cluster, 0) + precision\n",
    "        recalls[label] = recalls.get(label, 0) + recall\n",
    "\n",
    "    # Calculate overall precision and recall\n",
    "    overall_precision = sum(precisions.values()) / len(cluster_counts)\n",
    "    overall_recall = sum(recalls.values()) / len(label_counts)\n",
    "\n",
    "    # Calculate F1-score\n",
    "    fscore = 2 * (overall_precision * overall_recall) / (overall_precision + overall_recall) if (overall_precision + overall_recall) > 0 else 0\n",
    "\n",
    "    return overall_precision, overall_recall, fscore\n",
    "\n",
    "# Example usage\n",
    "hyp_obj_pairs = [\n",
    "    (1, 1), (1, 1), (1, 2), (2, 1), (3, -1), (-1, 2), (-1, 3)\n",
    "]\n",
    "\n",
    "precision, recall, fscore = bcubed_eval_bcu(hyp_obj_pairs)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F-score:\", fscore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import bcubed\n",
    "\n",
    "# def bcubed_eval(hyp_obj_pairs):\n",
    "#     \"\"\"\n",
    "#     Evaluate clustering using B-cubed metrics: precision and recall.\n",
    "\n",
    "#     Params:\n",
    "#         hyp_obj_pairs (list): List of tuples where:\n",
    "#             (i_pred_cluster, j_gt_label) - correct assignment\n",
    "#             (i_pred_cluster, -1) - false positive\n",
    "#             (-1, j_gt_label) - false negative\n",
    "\n",
    "#     Returns:\n",
    "#         tuple: (mean_precision, mean_recall)\n",
    "#     \"\"\"\n",
    "#     # Initialize dictionaries for predicted clusters and ground truth labels\n",
    "#     pred_clusters = {}\n",
    "#     gt_labels = {}\n",
    "\n",
    "#     # Populate dictionaries from the input list\n",
    "#     for cluster, label in hyp_obj_pairs:\n",
    "#         if cluster != -1:\n",
    "#             pred_clusters.setdefault(cluster, set()).add(label)\n",
    "#         if label != -1:\n",
    "#             gt_labels.setdefault(label, set()).add(cluster)\n",
    "\n",
    "#     # Calculate precision and recall using bcubed library\n",
    "#     precision = bcubed.precision(pred_clusters, gt_labels)\n",
    "#     recall = bcubed.recall(pred_clusters, gt_labels)\n",
    "\n",
    "#     return precision, recall\n",
    "\n",
    "# # Example usage\n",
    "# hyp_obj_pairs = [\n",
    "#     (1, \"A\"), (1, \"B\"), (2, \"A\"), (3, -1), (-1, \"B\"), (-1, \"C\")\n",
    "# ]\n",
    "\n",
    "# precision, recall = bcubed_eval(hyp_obj_pairs)\n",
    "# print(\"Precision:\", precision)\n",
    "# print(\"Recall:\", recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.hub.load('facebookresearch/detr', 'detr_resnet50', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "CUT_FRAMES = 5000\n",
    "\n",
    "# Function to read the JSON file\n",
    "def read_json_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "# Function to group frames by camera\n",
    "def group_frames_by_camera(frames):\n",
    "    camera_groups = defaultdict(list)\n",
    "    for frame in frames:\n",
    "        camera_name = frame['filename'].split('/')[-2]\n",
    "        camera_groups[camera_name].append(frame)\n",
    "    return camera_groups\n",
    "\n",
    "# Function to sort frames by timestamp\n",
    "def sort_frames_by_timestamp(frames):\n",
    "    return sorted(frames, key=lambda x: x['timestamp'])\n",
    "\n",
    "# Function to join frames into a video\n",
    "def join_frames_into_video(frames, output_video_path):\n",
    "    filepath = os.path.join('/ssd/r.musaev/deepsort/data/nuimages/', frames[0]['filename'])\n",
    "    frame = cv2.imread(filepath)\n",
    "    height, width, _ = frame.shape\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    \n",
    "    for frame_info in frames[:CUT_FRAMES]:\n",
    "        filepath = os.path.join('/ssd/r.musaev/deepsort/data/nuimages/', frame_info['filename'])\n",
    "\n",
    "        if not os.path.isfile(filepath):\n",
    "            frame = np.zeros_like(frame)\n",
    "        else:\n",
    "            frame = cv2.imread(filepath)\n",
    "            height, width, _ = frame.shape\n",
    "            out = cv2.VideoWriter(output_video_path, fourcc, 2, (width, height))    \n",
    "    \n",
    "        frame = cv2.imread(filepath)\n",
    "        out.write(frame)\n",
    "    \n",
    "    out.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the JSON file\n",
    "data = read_json_file('/ssd/r.musaev/deepsort/data/nuimages/v1.0-val/sample_data.json')\n",
    "\n",
    "# # Group frames by camera\n",
    "camera_groups = group_frames_by_camera(data)\n",
    "\n",
    "# # Process each camera group\n",
    "for camera_name, frames in camera_groups.items():\n",
    "    # Sort frames by timestamp\n",
    "    sorted_frames = sort_frames_by_timestamp(frames)\n",
    "    \n",
    "    # Join frames into a video\n",
    "    output_video_path = f'mini_{camera_name}_video.mp4'\n",
    "    join_frames_into_video(sorted_frames, output_video_path)\n",
    "    print(f'Video saved: {output_video_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read the JSON file\n",
    "# data = read_json_file('/ssd/r.musaev/deepsort/data/nuimages/v1.0-mini/sample_data.json')\n",
    "\n",
    "# # # Group frames by camera\n",
    "# camera_groups = group_frames_by_camera(data)\n",
    "\n",
    "# # # Process each camera group\n",
    "# for camera_name, frames in camera_groups.items():\n",
    "#     # Sort frames by timestamp\n",
    "#     sorted_frames = sort_frames_by_timestamp(frames)\n",
    "    \n",
    "#     # Join frames into a video\n",
    "#     output_video_path = f'mini_{camera_name}_video.mp4'\n",
    "#     join_frames_into_video(sorted_frames, output_video_path)\n",
    "#     print(f'Video saved: {output_video_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv('/ssd/r.musaev/deepsort/outputs/54/metrics_mot.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "camera_name = 'CAM_FRONT_LEFT'\n",
    "frame_pathes = sorted(glob.glob(f'/ssd/r.musaev/deepsort/data/nuimages/sweeps/{camera_name}/*'), key=lambda x: x.split('_')[-1])\n",
    "\n",
    "filepath = frame_pathes[0]\n",
    "frame = cv2.imread(filepath)\n",
    "height, width, _ = frame.shape\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "output_video_path = f'mini_{camera_name}_video.mp4'\n",
    "out = cv2.VideoWriter(output_video_path, fourcc, 2.0, (width, height))\n",
    "\n",
    "for frame_path in frame_pathes[:1000]:\n",
    "    frame = cv2.imread(frame_path)\n",
    "    out.write(frame)\n",
    "\n",
    "out.release()\n",
    "output_video_path\n",
    "\n",
    "len(frame_pathes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import os\n",
    "# from collections import defaultdict\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "# from pycocotools import mask as mask_utils\n",
    "\n",
    "# # Function to read the JSON file\n",
    "# def read_json_file(file_path):\n",
    "#     with open(file_path, 'r') as f:\n",
    "#         data = json.load(f)\n",
    "#     return data\n",
    "\n",
    "# # Function to group frames by camera\n",
    "# def group_frames_by_camera(frames):\n",
    "#     camera_groups = defaultdict(list)\n",
    "#     for frame in frames:\n",
    "#         camera_name = frame['filename'].split('/')[-2]\n",
    "#         camera_groups[camera_name].append(frame)\n",
    "#     return camera_groups\n",
    "\n",
    "# # Function to sort frames by timestamp\n",
    "# def sort_frames_by_timestamp(frames):\n",
    "#     return sorted(frames, key=lambda x: x['timestamp'])\n",
    "\n",
    "# # filter out class\n",
    "# def get_category_tokens(category_data, target_category):\n",
    "#     tokens = []    \n",
    "#     for category in category_data:\n",
    "#         if target_category in category['name']:\n",
    "#             tokens.append(category['token'])\n",
    "#     return tokens\n",
    "\n",
    "# # Function to join frames into a video with annotations\n",
    "# def join_frames_into_annotated_video(frames, annotations, category_meta, output_video_path):\n",
    "#     filepath = os.path.join('/ssd/r.musaev/deepsort/data/nuimages/', frames[0]['filename'])\n",
    "\n",
    "#     frame = cv2.imread(filepath)\n",
    "#     height, width, _ = frame.shape\n",
    "#     fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "#     out = cv2.VideoWriter(output_video_path, fourcc, 30.0, (width, height))\n",
    "\n",
    "#     # define category:\n",
    "#     category_tokens = get_category_tokens(category_meta, 'vehicle')\n",
    "    \n",
    "#     for frame_info in frames:\n",
    "#         filepath = os.path.join('/ssd/r.musaev/deepsort/data/nuimages/', frame_info['filename'])\n",
    "#         frame = cv2.imread(filepath)\n",
    "        \n",
    "#         # Retrieve annotations for the current frame\n",
    "#         frame_annotations = [ann for ann in annotations if ann['sample_data_token'] == frame_info['token'] and ann['category_token'] in category_tokens]\n",
    "        \n",
    "#         # Draw bounding boxes on the frame\n",
    "#         for ann in frame_annotations:\n",
    "#             bbox = ann['bbox']\n",
    "#             cv2.rectangle(frame, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0, 255, 0), 2)\n",
    "        \n",
    "#         out.write(frame)\n",
    "    \n",
    "#     out.release()\n",
    "\n",
    "\n",
    "# # Read the JSON files\n",
    "# frame_data = read_json_file('/ssd/r.musaev/deepsort/data/nuimages/v1.0-mini/sample_data.json')\n",
    "# annotation_data = read_json_file('/ssd/r.musaev/deepsort/data/nuimages/v1.0-mini/object_ann.json')\n",
    "# category_data = read_json_file('/ssd/r.musaev/deepsort/data/nuimages/v1.0-mini/category.json')\n",
    "\n",
    "# # Group frames by camera\n",
    "# camera_groups = group_frames_by_camera(frame_data)\n",
    "\n",
    "# # Process each camera group\n",
    "# for camera_name, frames in camera_groups.items():\n",
    "#     # Sort frames by timestamp\n",
    "#     sorted_frames = sort_frames_by_timestamp(frames)\n",
    "    \n",
    "#     # Join frames into an annotated video\n",
    "#     output_video_path = f'{camera_name}_annotated_video.mp4'\n",
    "#     join_frames_into_annotated_video(sorted_frames, annotation_data, category_data, output_video_path)\n",
    "#     print(f'Annotated video saved: {output_video_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "TOTAL_FRAMES = 1000\n",
    "\n",
    "def save_annotations_in_mot_format(frames, annotations, category_meta, output_mot_file):\n",
    "    with open(output_mot_file, 'w') as f:\n",
    "        # Write MOT header\n",
    "        # f.write(\"frame_num, object_id, x_min, y_min, width, height\\n\")\n",
    "        \n",
    "        # define category:\n",
    "        category_tokens = get_category_tokens(category_meta, 'vehicle')\n",
    "        \n",
    "        for frame_info in frames[:TOTAL_FRAMES]:\n",
    "            # Retrieve frame number\n",
    "            frame_num = frame_info['timestamp']  # Modify this according to your frame numbering\n",
    "            \n",
    "            # Retrieve annotations for the current frame and filter only vehicles\n",
    "            frame_annotations = [ann for ann in annotations if ann['sample_data_token'] == frame_info['token'] and ann['category_token'] in category_tokens]\n",
    "            \n",
    "            # Write annotations to the MOT file\n",
    "            for ann in frame_annotations:\n",
    "                bbox = ann['bbox']\n",
    "                object_id = ann['token']  # Use annotation token as object ID\n",
    "                x_min, y_min, width, height = bbox[0], bbox[1], bbox[2] - bbox[0], bbox[3] - bbox[1]\n",
    "                line = f\"{frame_num},{object_id},{x_min},{y_min},{width},{height}\\n\"\n",
    "                f.write(line)\n",
    "\n",
    "# Modify the function to include saving annotations in MOT format\n",
    "def join_frames_save_annotation(frames, annotations, category_meta, output_video_path, draw_boxes=False):\n",
    "    filepath = os.path.join('/ssd/r.musaev/deepsort/data/nuimages/', frames[0]['filename'])\n",
    "    frame = cv2.imread(filepath)\n",
    "    height, width, _ = frame.shape\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps=20, frameSize=(width, height))\n",
    "\n",
    "    # Define category:\n",
    "    category_tokens = get_category_tokens(category_meta, 'vehicle')\n",
    "    \n",
    "    # Specify the output MOT file path\n",
    "    output_mot_file = output_video_path.replace('.mp4', '.txt')\n",
    "\n",
    "    for frame_info in tqdm(frames[:TOTAL_FRAMES]):\n",
    "        filepath = os.path.join('/ssd/r.musaev/deepsort/data/nuimages/', frame_info['filename'])\n",
    "        if not os.path.isfile(filepath):\n",
    "            continue\n",
    "        frame = cv2.imread(filepath)\n",
    "        \n",
    "        if draw_boxes:\n",
    "            # Retrieve annotations for the current frame\n",
    "            frame_annotations = [ann for ann in annotations if ann['sample_data_token'] == frame_info['token'] and ann['category_token'] in category_tokens]\n",
    "            \n",
    "            # Draw bounding boxes on the frame\n",
    "            for ann in frame_annotations:\n",
    "                bbox = ann['bbox']\n",
    "                cv2.rectangle(frame, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0, 255, 0), 2)\n",
    "            \n",
    "        out.write(frame)\n",
    "    \n",
    "    # Save annotations in MOT format\n",
    "    save_annotations_in_mot_format(frames, annotations, category_meta, output_mot_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frame_annotations = [ann for ann in annotation_data if ann['sample_data_token'] == frame_data[0]['token']]\n",
    "# frame_annotations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_frames[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the JSON files\n",
    "frame_data = read_json_file('/ssd/r.musaev/deepsort/data/nuimages/v1.0-val/sample_data.json')\n",
    "annotation_data = read_json_file('/ssd/r.musaev/deepsort/data/nuimages/v1.0-val/object_ann.json')\n",
    "category_data = read_json_file('/ssd/r.musaev/deepsort/data/nuimages/v1.0-val/category.json')\n",
    "\n",
    "# Group frames by camera\n",
    "camera_groups = group_frames_by_camera(frame_data)\n",
    "\n",
    "# Process each camera group\n",
    "for camera_name, frames in camera_groups.items():\n",
    "    # Sort frames by timestamp\n",
    "    sorted_frames = sort_frames_by_timestamp(frames)\n",
    "    \n",
    "    # Join frames into an annotated video\n",
    "    output_video_path = f'/ssd/r.musaev/deepsort/input/nuimages/v1.0-test/{camera_name}.mp4'\n",
    "    join_frames_save_annotation(sorted_frames, annotation_data, category_data, output_video_path)\n",
    "    print(f'Annotated video saved: {output_video_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "gt_file = '/ssd/r.musaev/deepsort/outputs/57/S02c006_retinanet_resnet50_fpn_v2_torchreid_osnet_ibn_x1_0_imagenet.txt'\n",
    "df = pd.read_csv(gt_file, names=[\n",
    "                            'fr_num', 'id', 'bbleft', 'bbtop',\n",
    "                            'bbox_width', 'bbox_height', 'det_score',\n",
    "                            'class', 'visibility', 'stuff', 'desc_reid'], index_col=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "gt_file = '/ssd/r.musaev/deepsort/input/nuimages/v1.0-test/CAM_BACK_LEFT.txt'\n",
    "df = pd.read_csv(gt_file, names=[\n",
    "                            'fr_num', 'id', 'bbleft', 'bbtop',\n",
    "                            'bbox_width', 'bbox_height', 'det_score',\n",
    "                            'class', 'visibility', 'stuff', 'desc_reid'], index_col=False)\n",
    "df.fr_num.unique()                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "gt_file = '/ssd/r.musaev/deepsort/outputs/57/video4_retinanet_resnet50_fpn_v2_torchreid_osnet_ibn_x1_0_msmt17_combineall_256x128_amsgrad_ep150_stp60_lr0.0015_b64_fb10_softmax_labelsmooth_flip_jitter.txt'\n",
    "df = pd.read_csv(gt_file, names=[\n",
    "                            'fr_num', 'id', 'bbleft', 'bbtop',\n",
    "                            'bbox_width', 'bbox_height', 'det_score',\n",
    "                            'class', 'visibility', 'stuff', 'desc_reid'], index_col=False)\n",
    "df['desc_reid'] = df['desc_reid'].apply(lambda x: np.array(ast.literal_eval(x)))\n",
    "\n",
    "df_mcmt = pd.DataFrame(\n",
    "    columns=[\n",
    "        'fr_num', 'id', 'bbleft', 'bbtop',\n",
    "        'bbox_width', 'bbox_height', 'det_score', 'class'\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "num_camera = 0\n",
    "\n",
    "df['camera'] = num_camera\n",
    "\n",
    "tracks = [f\"{x}_c{num_camera}\" for x in df['id']]\n",
    "\n",
    "df['id'] = df['id'].apply(lambda x: f\"{x}_c{num_camera}\")\n",
    "\n",
    "df_sbst = df[['camera', 'id', 'bbleft', 'bbtop',\n",
    "                'bbox_width', 'bbox_height', 'det_score', 'class']]\n",
    "\n",
    "df_mcmt = pd.concat([df_mcmt, df_sbst])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = [\n",
    "    (1, 2),  # True positive\n",
    "    (1, 1),  # True positive\n",
    "    (2, 1),  # True positive\n",
    "    (2, 2),  # True positive\n",
    "    (3, -1), # False positive\n",
    "    (4, -1), # False positive\n",
    "    (5, -1), # False positive\n",
    "    (-1, 3), # False negative\n",
    "    (-1, 4), # False negative\n",
    "    (-1, 5)  # False negative\n",
    "]\n",
    "\n",
    "# Initialize dictionaries to store clusters and labels\n",
    "clusters = {}\n",
    "labels = {}\n",
    "\n",
    "# Fill in clusters and labels dictionaries\n",
    "for cluster, label in sample_data:\n",
    "    if cluster != -1:\n",
    "        if cluster in clusters:\n",
    "            clusters[cluster].append(label)\n",
    "        else:\n",
    "            clusters[cluster] = [label]\n",
    "    \n",
    "    if label != -1:\n",
    "        if label in labels:\n",
    "            labels[label].append(cluster)\n",
    "        else:\n",
    "            labels[label] = [cluster]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate precision\n",
    "precision = 0.0\n",
    "total_pairs = 0\n",
    "for cluster, label_list in clusters.items():\n",
    "    for label in label_list:\n",
    "        correct = label_list.count(label)\n",
    "        total_pairs += correct\n",
    "        precision += correct / len(label_list)\n",
    "\n",
    "precision /= total_pairs\n",
    "\n",
    "# Calculate recall\n",
    "recall = 0.0\n",
    "total_pairs = 0\n",
    "for label, cluster_list in labels.items():\n",
    "    for cluster in cluster_list:\n",
    "        correct = cluster_list.count(cluster)\n",
    "        total_pairs += correct\n",
    "        recall += correct / len(cluster_list)\n",
    "\n",
    "recall /= total_pairs\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bcubed_eval(sample_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_roi(path):\n",
    "    img = Image.open(path, mode='r')\n",
    "    img.load()\n",
    "    if img.size[0] > img.size[1]:\n",
    "        img = img.transpose(Image.TRANSPOSE)\n",
    "        \n",
    "    im = np.asarray(img, dtype=\"uint8\")\n",
    "    if im.shape[0] > im.shape[1]:\n",
    "        im = im.T\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def bcubed_eval(hyp_obj_pairs):\n",
    "    \"\"\"cluster evaluation function\n",
    "    Based on https://link.springer.com/article/10.1007/s10791-008-9066-8\n",
    "\n",
    "    Params\n",
    "    ------\n",
    "    hyp_obj_pairs: list\n",
    "        list of tuples where:\n",
    "            (i_pred_cluster, j_gt_label) - correct assignment\n",
    "            (i_pred_cluster, -1) - fp\n",
    "            (-1, j_gt_label) - fn\n",
    "\n",
    "    Returns\n",
    "    ------\n",
    "    (mean_p, mean_r): tuple\n",
    "        mean (precision, recall) per threshold\n",
    "    \"\"\"\n",
    "\n",
    "    AVERAGE_PRECISION = 0.0\n",
    "    AVERAGE_RECALL = 0.0\n",
    "    AVERAGE_FP = 0.0\n",
    "    recall_items = 0\n",
    "    precision_items = 0\n",
    "    used_pairs = set()\n",
    "\n",
    "    hyp_obj_pairs = sorted(hyp_obj_pairs)\n",
    "    for pair in tqdm(hyp_obj_pairs):\n",
    "        if pair in used_pairs:\n",
    "            continue\n",
    "\n",
    "        tp = 0\n",
    "        same_cluster = 0\n",
    "        same_label = 0\n",
    "\n",
    "        if pair[0] == pair[1] == -1:\n",
    "            print('\\n\\n\\n\\n\\n')\n",
    "            print('pair[0] == pair[0] == -1')\n",
    "            print('\\n\\n\\n\\n\\n')\n",
    "\n",
    "        if pair[1] == -1:  # FP\n",
    "            AVERAGE_PRECISION += 0\n",
    "            precision_items += 1\n",
    "            AVERAGE_FP += 1\n",
    "            continue\n",
    "\n",
    "        if pair[0] == -1:  # FN\n",
    "            AVERAGE_RECALL += 0\n",
    "            recall_items += 1\n",
    "            continue\n",
    "\n",
    "        for item in hyp_obj_pairs:\n",
    "            if item[0] == pair[0]:\n",
    "                same_cluster += 1\n",
    "                if item[1] == pair[1]:\n",
    "                    tp += 1\n",
    "\n",
    "            if item[1] == pair[1]:\n",
    "                same_label += 1\n",
    "\n",
    "        p = tp / same_cluster\n",
    "        assert p <= 1\n",
    "        AVERAGE_PRECISION += p * tp\n",
    "        precision_items += tp\n",
    "\n",
    "        r = tp / same_label\n",
    "        assert r <= 1\n",
    "        AVERAGE_RECALL += r * tp\n",
    "        recall_items += tp\n",
    "        used_pairs.add(pair)\n",
    "\n",
    "    mean_p = AVERAGE_PRECISION / precision_items\n",
    "    mean_r = AVERAGE_RECALL / recall_items\n",
    "    mean_fp_rate = AVERAGE_FP / precision_items\n",
    "    print(\"mean_fp_rate\", mean_fp_rate, \"1-mean_p\", 1-mean_p, \"mean_p\", mean_p)\n",
    "    return (mean_p, mean_r, mean_fp_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# draw gt boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "\n",
    "video_path = '/ssd/r.musaev/deepsort/input/AIC22_Track1_MTMC_Tracking/S02c007.mp4'\n",
    "out_dir = '/ssd/r.musaev/deepsort/input/'\n",
    "\n",
    "# Video stream\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "frame_width = int(cap.get(3))\n",
    "frame_height = int(cap.get(4))\n",
    "frame_fps = int(cap.get(5))\n",
    "frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "save_name = video_path.split(os.path.sep)[-1].split('.')[0]\n",
    "\n",
    "# Define codec and create VideoWriter object\n",
    "out = cv2.VideoWriter(\n",
    "    f\"{out_dir}/{save_name}_annotated.mp4\",\n",
    "    cv2.VideoWriter_fourcc(*'mp4v'), frame_fps,\n",
    "    (frame_width, frame_height)\n",
    ")\n",
    "\n",
    "frame_count = 0  # To count total frames\n",
    "total_fps = 0  # To get the final frames per second\n",
    "\n",
    "gt_camera = pd.read_csv(video_path.replace('mp4', 'txt'), names=[\n",
    "    'fr_num', 'id', 'bbleft', 'bbtop',\n",
    "    'bbox_width', 'bbox_height', 'det_score',\n",
    "    'class', 'visibility', 'stuff'],\n",
    "                    index_col=False)\n",
    "\n",
    "while cap.isOpened():\n",
    "    # Read a frame\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        frame_count += 1\n",
    "\n",
    "        # Get ground truth bounding boxes for the current frame\n",
    "        gt_bboxes = gt_camera[(gt_camera['fr_num'] == frame_count)]\n",
    "        \n",
    "        # Draw ground truth bounding boxes\n",
    "        for _, bbox in gt_bboxes.iterrows():\n",
    "            left = int(bbox['bbleft'])\n",
    "            top = int(bbox['bbtop'])\n",
    "            width = int(bbox['bbox_width'])\n",
    "            height = int(bbox['bbox_height'])\n",
    "            cv2.rectangle(frame, (left, top), (left + width, top + height), (0, 255, 0), 2)\n",
    "\n",
    "        # Write the annotated frame to the output video\n",
    "        out.write(frame)\n",
    "\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "def create_video_from_images(image_folder, output_video_path, fps=30):\n",
    "    # Get the list of images in the folder\n",
    "    images = [img for img in os.listdir(image_folder) if img.endswith(\".png\")]\n",
    "    frame = cv2.imread(os.path.join(image_folder, images[0]))\n",
    "    height, width, layers = frame.shape\n",
    "\n",
    "    # Define the codec and create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Be sure to use the lower case\n",
    "    video = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "\n",
    "    for image in images:\n",
    "        video.write(cv2.imread(os.path.join(image_folder, image)))\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    video.release()\n",
    "\n",
    "# Specify input image folder and output video path\n",
    "image_folder = '/ssd/r.musaev/deepsort/data/Wildtrack_dataset/Image_subsets/C1'\n",
    "output_video_path = '/ssd/r.musaev/deepsort/data/Wildtrack_dataset/Image_subsets/C1.mp4'\n",
    "\n",
    "# Create the video from images\n",
    "create_video_from_images(image_folder, output_video_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIC22_Track1_MTMC_Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/ssd/r.musaev/deepsort/data/AIC22_Track1_MTMC_Tracking/eval/ground_truth_validation.txt', names=[\n",
    "    'camera', 'fr_num', 'id', 'bbleft', 'bbtop',\n",
    "    'bbox_width', 'bbox_height', 'det_score',\n",
    "    'class',], index_col=False, sep=' ')\n",
    "\n",
    "\n",
    "df = df[df.camera.apply(lambda x: x in [6, 7, 8, 9])]\n",
    "df.to_csv('/ssd/r.musaev/deepsort/data/AIC22_Track1_MTMC_Tracking/eval/ground_truth_validation_S02.txt', header=None, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'open-mmlab' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n open-mmlab ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import mmcv\n",
    "from mmcv import Config\n",
    "from mmtracking import init_dist, inference_detector, show_result_pyplot, inference_track\n",
    "from mmtracking.models import build_tracker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mmcv\n",
    "from mmcv import Config\n",
    "from mmtracking import init_dist, inference_detector, show_result_pyplot, inference_track\n",
    "from mmtracking.models import build_tracker\n",
    "\n",
    "# Step 1: Prepare your data\n",
    "# Make sure your data is organized appropriately with necessary annotations.\n",
    "\n",
    "# Step 2: Install mmtracking\n",
    "# Install mmtracking and its dependencies using pip.\n",
    "\n",
    "# Step 3: Configure the tracker\n",
    "# Load the tracker configuration file\n",
    "config_file = 'path/to/your/tracker_config.py'\n",
    "cfg = Config.fromfile(config_file)\n",
    "\n",
    "# Build the tracker model\n",
    "model = build_tracker(cfg.model)\n",
    "\n",
    "# Load model checkpoint\n",
    "checkpoint_file = 'path/to/your/checkpoint.pth'\n",
    "model.load_state_dict(mmcv.load(checkpoint_file)['state_dict'])\n",
    "\n",
    "# Step 4: Run the tracker\n",
    "# Initialize distributed computing environment if necessary\n",
    "init_dist('cpu', port=29500)\n",
    "\n",
    "# Load your video data\n",
    "video_file = 'path/to/your/video.mp4'\n",
    "video = mmcv.VideoReader(video_file)\n",
    "\n",
    "# Run inference on the video\n",
    "results = []\n",
    "for frame in video:\n",
    "    result = inference_track(model, frame)\n",
    "    results.append(result)\n",
    "\n",
    "# Visualize the tracking results\n",
    "for frame, result in zip(video, results):\n",
    "    show_result_pyplot(model, frame, result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
